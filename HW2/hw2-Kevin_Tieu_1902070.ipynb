{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2, CHEE 6397 (Data-Driven Materials Modeling)\n",
    "\n",
    "#### **Topics**: Committee Models & Cross-Validation\n",
    "\n",
    "#### **Due**: October 17, 2023\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "- The file you submit should be named as `hw2-<FirstName>_<LastName>-<UHID>.ipynb`, e.g. `hw2-Mingjian_Wen-00001111.ipynb`.\n",
    "- Input your answer to each question in the `Answer` cell. Feel free add to more cells as needed.\n",
    "\n",
    "### Markdown and Math\n",
    "\n",
    "- You can use `markdown` cell to type the text part of your answers to a question. And math equations can be typed using LaTex.\n",
    "- See https://gtribello.github.io/mathNET/assets/notebook-writing.html for a quick intro to markdown in Jupyter and how to write math equations.\n",
    "- See https://ia.wikipedia.org/wiki/Wikipedia:LaTeX_symbols for a list of LaTex symbols.\n",
    "- If you are more used to MS Word equation typesetting, you can find a graphical LaTex equations generator at: https://latexeditor.lagrida.com\n",
    "\n",
    "### Code\n",
    "\n",
    "- Your Python code to a question can be written using the `code` cell.\n",
    "- Make sure all used packages are imported properly. For example, before submission, do `Kernel->Restart and Run All Cells...` from the menu bar to double check. If we cannot run your notebook, we won't be able to grade it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scores\n",
    "\n",
    "Problem 1\n",
    "\n",
    "- a [10 points]:\n",
    "- b [10 points]:\n",
    "\n",
    "Problem 2\n",
    "\n",
    "- a [10 points]:\n",
    "- b [10 points]:\n",
    "- c [10 points]:\n",
    "- d [10 points]:\n",
    "\n",
    "Problem 3\n",
    "\n",
    "- a [10 points]:\n",
    "- b [10 points]:\n",
    "- c [10 points]:\n",
    "- d [10 points]:\n",
    "\n",
    "Total: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Gradient boosting` is a type of additive models that uses an ensemble of weak learners to make predictions. Instead of solving the complex minimization problem directly, it solves a sequence of simpler problems. The weak learners are added to the ensemble in a forward stage-wise manner. At each stage, a weak learner is added to the ensemble to minimize the loss function of the whole ensemble.\n",
    "- (1) Briefly explain what is a weak learner.\n",
    "- (2) What are the similarities and differences between `gradient boosting` and `AdaBoost`?\n",
    "- (3) For regression using `gradient boosting`, if a loss function based on the difference between the target and the prediction is used (e.g. the squared error $L = [t - f(x)]^2$, where $t$ is the target and $f(x)$ is model prediction), what are the best solutions for the weak learners? Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "* (1) Weak learner is a simple model or algorithm that performs slightly better than random guessing ($error~probability < 1/2$).\n",
    "\n",
    "* (2)\n",
    "    * Similarities:\n",
    "        * Utilize and combine multiple weak learners to produce a strong learner (committee-based models)\n",
    "        * Train weak learners sequentially, in which new learners are trained to compensate the errors made by the previous ones.\n",
    "        * Use assigned weights on feature data, especially with features that were incorrectly predicted (high error rate).\n",
    "    \n",
    "    * Differences:\n",
    "        * Tree Depth: `AdaBoost` often uses shallow trees (stumps) while `Gradient Boosting` can have much deeper decision trees\n",
    "        * Weight assignment:\n",
    "            * `AdaBoost` updates weights to each training instance in each iteration, with the weight of misclassified instances increased and that of correctly classified ones decreased.\n",
    "            * `Gradient Boosting` updates the weights each iteration in order to reduce the value of the loss function.\n",
    "        * Error: `AdaBoost`'s final model are built sequentially, with new learners made by taking the previous learners' errors into account. `Gradient Boosting`'s final model is the additive combination of all the weak learners.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (3) The loss function is $L = (t - f(x))^2$. The best solutions to minimize the loss function for the weak learners are:\n",
    "$$ (\\beta _m, \\gamma _m) = argmin_{\\beta,\\gamma} \\sum _{i=1} ^N L(y_i,f_{m-1}(x_i) + \\beta b(x_i,\\gamma)) $$\n",
    "$$ (\\beta _m, \\gamma _m) = argmin_{\\gamma} \\sum _{i=1} ^N (y_i - f_{m-1}(x_i) - \\beta b(x_i,\\gamma))^2 $$\n",
    "$$ \\rightarrow \\frac {\\delta}{\\delta \\gamma}\\sum _{i=1} ^N (y_i - f_{m-1}(x_i) - \\beta b(x_i,\\gamma))^2 = 0$$\n",
    "$$ -2 \\sum _{i=1} ^N (y_i - f_{m-1}(x_i) - \\beta b(x_i,\\gamma)) [\\frac {\\delta}{\\delta \\gamma} (\\beta b(x_i,\\gamma))] = 0$$\n",
    "\n",
    "$$\\rightarrow \\beta b(x_i,\\gamma) = \\frac{1}{n} \\sum _{i=1} ^N (y_i - f_{m-1}(x_i))$$\n",
    "$$or~~~ \\beta b(x_i,\\gamma) = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Random forest` is a type of `bagging` algorithm that uses an ensemble of decision trees to make predictions, and each decision tree is fit to a separate bootstrapped dataset. \n",
    "- (1) What are the major differences between `random forest` and `bagging with vanilla decision trees`? Vanilla decision tree is the algorithm discussed in `Lecture 5`.\n",
    "- (2) What are the advantages of using `random forest` over `bagging with vanilla decision trees`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "* (1) `Random forest` and `bagging trees` all take advantage of bootstrap dataset. However, `random forest` selects a random subset of features for each tree, while `bagging tree` use all features for each tree.\n",
    "\n",
    "* (2) `Random forest` uses randomized bootstrapping and feature selection, making each tree independent from one another. As a result, it has lower bias and reduce overfitting (a typical problem with decision trees). This leads to improved generalization and better accuracy than `bagging`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, we will use the `experimental formation enthalpy` dataset of inorganic materials to explore the `random forest` and `gradient boosting` algorithms, as well as the `cross-validation` technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is provided in the file `experimental_formation_enthalpy.csv`. Here is some explanation: \n",
    "- any column starting with `Magpie` is a feature\n",
    "- the column `formation_enthalpy` is the target\n",
    "- the column `formula` gives the chemical formula of the material, from which the features are obtained using the `matminer` package composition descriptor discussed in Lab 4. \n",
    "- the column `mpid` gives the [Materials Project](https://materialsproject.org/) ID of the material. You can know more about the material by searching the ID on the Materials Project website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load the dataset into a `pandas` dataframe.\n",
    "- There are `NaN` (Not a Number) values in the target `formation_enthalpy` column. Clean the dataset by removing the rows with `NaN` values in the target column. Note, other columns like `mpid` also have `NaN` values, but you should not remove the rows with `NaN` values in those columns.\n",
    "- Convert all the features to a 2D numpy array `X` and the target to a 1D numpy array `y`.\n",
    "\n",
    "Hints: refer to `Lab 3` and `HW 1` on how to do this.\n",
    " \n",
    "Then answer the following questions:\n",
    "1. How many samples (data points) are there in the dataset?\n",
    "2. How many features are there in the dataset?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define macro\n",
    "TARGET = 'formation_enthalpy'\n",
    "FEATURE_PREFIX = 'Magpie'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset into a pandas dataframe\n",
    "df = pd.read_csv(\"experimental_formation_enthalpy.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the rows with NaN values for formation_enthalpy\n",
    "df = df.dropna(axis=0,subset=TARGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert target and features set to numpy arrays\n",
    "target_set = df[TARGET].to_numpy()\n",
    "features_set = df.loc[:,df.columns.str.contains(FEATURE_PREFIX)].to_numpy()\n",
    "features_names = df.columns[df.columns.str.contains(FEATURE_PREFIX)].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of valid samples in the dataset: 1196\n",
      "Number of features in the dataset: 132\n"
     ]
    }
   ],
   "source": [
    "# NUMBER OF SAMPLES IN THE DATASET\n",
    "print('Number of valid samples in the dataset:', features_set.shape[0])\n",
    "# NUMBER OF FEATURES IN THE DATASET\n",
    "print('Number of features in the dataset:', features_set.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Briefly explain what is `cross-validation` and why it is useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer  \n",
    "\n",
    "Cross validation divides the available data into multiple folds and use of one fold as a validation set while the rest are used for training. The process is repeated several times, each with different a validation set. The average performance of all validation iterations is the estimate of the model performance.\n",
    "\n",
    "Cross validation is useful because it prevents overfitting by training evaluating the model based on different training and validation sets. This is particularly beneficial on small dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a random forest model to the `training set` using 5-fold cross validation, and evaluate the training and test `root mean squared errors`. \n",
    "\n",
    "Hints:\n",
    "\n",
    "- Use `RandomForestRegressor` in the `sklearn.ensemble` module as the estimator.\n",
    "- Use `cross_validate` in the `sklearn.model_selection` module to perform cross validation.\n",
    "- You can use `neg_root_mean_squared_error` as the `scoring` function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestRegressor()\n",
    "cv_results = cross_validate(rf_model, features_set, target_set, cv = 5,\n",
    "                            scoring='neg_root_mean_squared_error',\n",
    "                            return_train_score=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d)\n",
    "\n",
    "\n",
    "Then answer the below questions:\n",
    "1. What are the mean and standard deviation of the `training errors` from the 5-fold cross validation? \n",
    "2. What are the mean and standard deviation of the `test errors` from the 5-fold cross validation? \n",
    "3. Is the model sensitive to data splitting? Justify your answer.\n",
    "4. Do you think random forest model is a good model for this dataset? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training errors:\n",
      "\tMean = -0.03621601728782425\n",
      "\tStandard deviation = 0.0009847859975011459\n",
      "Test errors:\n",
      "\tMean = -0.1719672837585516\n",
      "\tStandard deviation = 0.017310920889979584\n"
     ]
    }
   ],
   "source": [
    "# 1 \n",
    "print('Training errors:\\n\\tMean = {}\\n\\tStandard deviation = {}'.format(cv_results['train_score'].mean(),cv_results['train_score'].std()))\n",
    "# 2\n",
    "print('Test errors:\\n\\tMean = {}\\n\\tStandard deviation = {}'.format(cv_results['test_score'].mean(),cv_results['test_score'].std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **(3)** The model is not sensitive to data splitting. Since the standard deviation of the training and test scores is about 5% and 11% of the mean, respectively, the differences in performance for various folds/datasets are not significant.\n",
    "\n",
    "* **(4)** Random forest is a good model for this dataset because the RMSE is much lower than the mean of the target data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The `experimental formation enthalpy` dataset consits of more than 100 features; some of them are definitely more important than others. In this problem, we explore the feature importance using `gradient boosting`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This feature importance we've discussed in class is called `impurity based feature importance`. There is another type of feature importance called `permutation feature importance`.\n",
    "- Briefly describe what is `impurity based feature importance` and steps to compute it.\n",
    "- Briefly describe what is `permutation feature importance` and steps to compute it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `Impurity-based feature importance` is a technique used to evaluate the significance of each feature in a dataset to the model's prediction. It quantifies the influence of each feature in reducing error in the decision tree. Calculating steps:\n",
    "    * Grow a decision tree\n",
    "    * Calculate impurity of each leaf (Gini impurity for classification or error for regression)\n",
    "    * Calculate total impurity = weighted average of impurities for all leaves\n",
    "\n",
    "* `Permutation feature importance` is a technique used to evaluate the significant of each feature in a dataset to the model's prediction. It measures the impact of randomly shuffling the values of a certain feature while keeping other features unchanged.\n",
    "    * Evaluate the initial performance of the model (unshuffled)\n",
    "    * Select a feature to compute the importance score\n",
    "    * Randomly shuffle the values of the selected feature across all data points in the dataset.\n",
    "    * Evaluate the model after permutation by calculate the same metrics in the initial evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Split the features dataset ( features `X` and target `y`) in problem 2 into a training set and a test set with a 8:2 ratio. You should get `X_train`, `X_test`, `y_train`,  and `y_test`.\n",
    "- Fit a `gradient boosting` model to the `training set`, and evaluate the training and test `mean squared errors`. You can use `scklearn`'s `GradientBoostingRegressor`.\n",
    "- What are the training and test `mean squared errors`? \n",
    "- How is the model performance compared to the random forest model in problem 2?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a gradient boosting fitting function for easy testing and training\n",
    "def GradientBoostModeling(features, target, test_ratio = 0.2):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features,target, test_size=test_ratio, random_state= 17)\n",
    "\n",
    "    model = GradientBoostingRegressor()\n",
    "    model.fit(X_train,y_train)\n",
    "\n",
    "    # training set error\n",
    "    y_pred = model.predict(X_train)\n",
    "    mse = mean_squared_error(y_train,y_pred)\n",
    "    print(\"MSE on the training set: \", mse)\n",
    "\n",
    "    # test set error\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test,y_pred)\n",
    "    print(\"MSE on the test set: \", mse)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on the training set:  0.003766923157598716\n",
      "MSE on the test set:  0.006919952438089583\n"
     ]
    }
   ],
   "source": [
    "gb_model = GradientBoostModeling(features_set, target_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Rightarrow$ The MSE is much lower than the RMSE from random forest, meaning the gradient boosting model performs better than random forest for this dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c)\n",
    "- Obtain the feature importance from the `gradient boosting` model in (b). Which type of feature importance is it?\n",
    "- What are the 10 most important features, and what are the 10 least important features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature performance is the impurity-based feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = gb_model.feature_importances_\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "feature_importance_name = np.array(features_names)[sorted_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 MOST important features: \n",
      "\t MagpieData minimum NfUnfilled\n",
      "\t MagpieData mode GSmagmom\n",
      "\t MagpieData minimum NpValence\n",
      "\t MagpieData mode NsValence\n",
      "\t MagpieData range NsValence\n",
      "\t MagpieData maximum NsValence\n",
      "\t MagpieData maximum Column\n",
      "\t MagpieData range GSmagmom\n",
      "\t MagpieData maximum GSmagmom\n",
      "\t MagpieData minimum GSmagmom\n",
      "\n",
      "10 LEAST important features: \n",
      "\t MagpieData avg_dev Electronegativity\n",
      "\t MagpieData minimum GSvolume_pa\n",
      "\t MagpieData maximum Electronegativity\n",
      "\t MagpieData avg_dev NdValence\n",
      "\t MagpieData avg_dev NpUnfilled\n",
      "\t MagpieData range Column\n",
      "\t MagpieData mode GSvolume_pa\n",
      "\t MagpieData minimum Number\n",
      "\t MagpieData minimum CovalentRadius\n",
      "\t MagpieData mean NpUnfilled\n"
     ]
    }
   ],
   "source": [
    "print(\"10 MOST important features: \")\n",
    "for i in range(0,10):\n",
    "    print(\"\\t\",feature_importance_name[i])\n",
    "\n",
    "print(\"\\n10 LEAST important features: \")\n",
    "for i in range(0,10):\n",
    "    print(\"\\t\",feature_importance_name[-1-i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### (d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the 100 least important features from the dataset, and repeat step (b).\n",
    "\n",
    "- What are the training and test `mean squared errors`?\n",
    "- Compare the results with those in (b). What happens to the test error? Any explanation?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete 100 least important features\n",
    "new_feature_set = np.delete(features_set,sorted_idx[-1:-101:-1],1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on the training set:  0.01698344628502337\n",
      "MSE on the test set:  0.024212839094280927\n"
     ]
    }
   ],
   "source": [
    "# train and test the new model\n",
    "new_gb_model = GradientBoostModeling(new_feature_set,target_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to **(b)**, the MSE of the new dataset is considerably higher, indicating this dataset performs much worse than the initial dataset. Since 100 out of 132 features have been removed from the dataset, their weighted factors have been lost as well. Even though these are less important/impacting than the remaining features, in aggregate they still contribute significantly to the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e)  NO need to do this for HW2\n",
    "\n",
    "- You might want to explore the feature importance using `permutation feature importance`, and compare the results with `impurity based feature importance`.\n",
    "- You might want to compare the feature importance between `random forest` and `gradient boosting`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
