{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2, CHEE 6397 (Data-Driven Materials Modeling)\n",
    "\n",
    "#### **Topics**: Committee Models & Cross-Validation\n",
    "\n",
    "#### **Due**: October 17, 2023\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "- The file you submit should be named as `hw2-<FirstName>_<LastName>-<UHID>.ipynb`, e.g. `hw2-Mingjian_Wen-00001111.ipynb`.\n",
    "- Input your answer to each question in the `Answer` cell. Feel free add to more cells as needed.\n",
    "\n",
    "### Markdown and Math\n",
    "\n",
    "- You can use `markdown` cell to type the text part of your answers to a question. And math equations can be typed using LaTex.\n",
    "- See https://gtribello.github.io/mathNET/assets/notebook-writing.html for a quick intro to markdown in Jupyter and how to write math equations.\n",
    "- See https://ia.wikipedia.org/wiki/Wikipedia:LaTeX_symbols for a list of LaTex symbols.\n",
    "- If you are more used to MS Word equation typesetting, you can find a graphical LaTex equations generator at: https://latexeditor.lagrida.com\n",
    "\n",
    "### Code\n",
    "\n",
    "- Your Python code to a question can be written using the `code` cell.\n",
    "- Make sure all used packages are imported properly. For example, before submission, do `Kernel->Restart and Run All Cells...` from the menu bar to double check. If we cannot run your notebook, we won't be able to grade it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scores\n",
    "\n",
    "Problem 1\n",
    "\n",
    "- a [10 points]:\n",
    "- b [10 points]:\n",
    "\n",
    "Problem 2\n",
    "\n",
    "- a [10 points]:\n",
    "- b [10 points]:\n",
    "- c [10 points]:\n",
    "- d [10 points]:\n",
    "\n",
    "Problem 3\n",
    "\n",
    "- a [10 points]:\n",
    "- b [10 points]:\n",
    "- c [10 points]:\n",
    "- d [10 points]:\n",
    "\n",
    "Total: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Gradient boosting` is a type of additive models that uses an ensemble of weak learners to make predictions. Instead of solving the complex minimization problem directly, it solves a sequence of simpler problems. The weak learners are added to the ensemble in a forward stage-wise manner. At each stage, a weak learner is added to the ensemble to minimize the loss function of the whole ensemble.\n",
    "- (1) Briefly explain what is a weak learner.\n",
    "- (2) What are the similarities and differences between `gradient boosting` and `AdaBoost`?\n",
    "- (3) For regression using `gradient boosting`, if a loss function based on the difference between the target and the prediction is used (e.g. the squared error $L = [t - f(x)]^2$, where $t$ is the target and $f(x)$ is model prediction), what are the best solutions for the weak learners? Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Random forest` is a type of `bagging` algorithm that uses an ensemble of decision trees to make predictions, and each decision tree is fit to a separate bootstrapped dataset. \n",
    "- (1) What are the major differences between `random forest` and `bagging with vanilla decision trees`? Vanilla decision tree is the algorithm discussed in `Lecture 5`.\n",
    "- (2) What are the advantages of using `radom forest` over `bagging with vanilla decision trees`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, we will use the `experimental formation enthalpy` dataset of inorganic materials to explore the `random forest` and `gradient boosting` algorithms, as well as the `cross-validation` technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is provided in the file `experimental_formation_enthalpy.cxv`. Here is some explanation: \n",
    "- any column starting with `Magpie` is a feature\n",
    "- the column `formation_enthalpy` is the target\n",
    "- the column `formula` gives the chemical formula of the material, from which the features are obtained using the `matminer` package composition descriptor discussed in Lab 4. \n",
    "- the column `mpid` gives the [Materials Project](https://materialsproject.org/) ID of the material. You can know more about the material by searching the ID on the Materials Project website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load the dataset into a `pandas` dataframe.\n",
    "- There are `NaN` (Not a Number) values in the target `formation_enthalpy` column. Clean the dataset by removing the rows with `NaN` values in the target column. Note, other columns like `mpid` also have `NaN` values, but you should not remove the rows with `NaN` values in those columns.\n",
    "- Convert all the features to a 2D numpy array `X` and the target to a 1D numpy array `y`.\n",
    "\n",
    "Hints: refer to `Lab 3` and `HW 1` on how to do this.\n",
    " \n",
    "Then answer the following questions:\n",
    "1. How many samples (data points) are there in the dataset?\n",
    "2. How many features are there in the dataset?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Briefly explain what is `cross-validation` and why it is useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a random forest model to the `training set` using 5-fold cross validation, and evaluate the training and test `root mean squared errors`. \n",
    "\n",
    "Hints:\n",
    "\n",
    "- Use `RandomForestRegressor` in the `sklearn.ensemble` module as the estimator.\n",
    "- Use `cross_validate` in the `sklearn.model_selection` module to perform cross validation.\n",
    "- You can use `neg_root_mean_squared_error` as the `scoring` function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d)\n",
    "\n",
    "\n",
    "Then answer the below questions:\n",
    "1. What are the mean and standard deviation of the `training errors` from the 5-fold cross validation? \n",
    "2. What are the mean and standard deviation of the `test errors` from the 5-fold cross validation? \n",
    "3. Is the model sensitive to data splitting? Justify your answer.\n",
    "4. Do you think random forest model is a good model for this dataset? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The `experimental formation enthalpy` dataset consits of more than 100 features; some of them are definitely more important than others. In this problem, we explore the feature importance using `gradient boosting`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This feature importance we've discussed in class is called `impurity based feature importance`. There is another type of feature importance called `permutation feature importance`.\n",
    "- Briefly describe what is `impurity based feature importance` and steps to compute it.\n",
    "- Briefly describe what is `permutation feature importance` and steps to compute it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Split the features dataset ( features `X` and target `y`) in problem 2 into a training set and a test set with a 8:2 ratio. You should get `X_train`, `X_test`, `y_train`,  and `y_test`.\n",
    "- Fit a `gradient boosting` model to the `training set`, and evaluate the training and test `mean squared errors`. You can use `scklearn`'s `GradientBoostingRegressor`.\n",
    "- What are the training and test `mean squared errors`? \n",
    "- How is the model performance compared to the random forest model in problem 2?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c)\n",
    "- Obtain the feature importance from the `gradient boosting` model in (b). Which type of feature importance is it?\n",
    "- What are the 10 most important features, and what are the 10 least important features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### (d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the 100 least important features from the dataset, and repeat step (b).\n",
    "\n",
    "- What are the training and test `mean squared errors`?\n",
    "- Compare the results with those in (b). What happens to the test error? Any explanation?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e)  NO need to do this for HW2\n",
    "\n",
    "- You might want to explore the feature importance using `permutation feature importance`, and compare the results with `impurity based feature importance`.\n",
    "- You might want to compare the feature importance between `random forest` and `gradient boosting`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
