{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW4, CHEE 6397 (Data-Driven Materials Modeling)\n",
    "\n",
    "#### **Topics**: Deep Learning\n",
    "\n",
    "#### **Due**: December 7, 2023\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "- The file you submit should be named as `hw4-<FirstName>_<LastName>-<UHID>.ipynb`, e.g. `hw4-Mingjian_Wen-00001111.ipynb`.\n",
    "- Input your answer to each question in the `Answer` cell. Feel free add to more cells as needed.\n",
    "\n",
    "### Markdown and Math\n",
    "\n",
    "- You can use `markdown` cell to type the text part of your answers to a question. And math equations can be typed using LaTex.\n",
    "- See https://gtribello.github.io/mathNET/assets/notebook-writing.html for a quick intro to markdown in Jupyter and how to write math equations.\n",
    "- See https://ia.wikipedia.org/wiki/Wikipedia:LaTeX_symbols for a list of LaTex symbols.\n",
    "- If you are more used to MS Word equation typesetting, you can find a graphical LaTex equations generator at: https://latexeditor.lagrida.com\n",
    "\n",
    "### Code\n",
    "\n",
    "- Your Python code to a question can be written using the `code` cell.\n",
    "- Make sure all used packages are imported properly. For example, before submission, do `Kernel->Restart and Run All Cells...` from the menu bar to double check. If we cannot run your notebook, we won't be able to grade it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scores\n",
    "\n",
    "- a [5 points]: \n",
    "- b [10 points]:\n",
    "- c [5 points]:\n",
    "- d [10 points]:\n",
    "- e [15 points]:\n",
    "- f [10 points]:\n",
    "- g [20 points]:\n",
    "- h [10 points]:\n",
    "- i [10 points]: \n",
    "- j [5 points]:  \n",
    "\n",
    "Total: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem\n",
    "\n",
    "In this problem, we will train an MLP for the classification of the reaction reaction family data used in HW3.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)\n",
    "\n",
    "Load your dataset features as `X` and labels (reaction family) as `y`. Convert them into PyTorch `tensors`.\n",
    "What are the **data type** of `X` and `y`?\n",
    "\n",
    "Note, DON'T use the data file from HW3. (I've changed the label be to 0-based in HW 4 from 1-based in HW3.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)\n",
    "\n",
    "- Plot a histogram of the number of samples in each reaction family.\n",
    "- How many different reaction families are there in the dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (C)\n",
    "\n",
    "Split the dataset into training/validation/test sets with a ratio of 0.8/0.1/0.1. After the split, you should have `X_train`, `y_train`, `X_val`, `y_val`, `X_test`, and `y_test`.\n",
    "Hint: \n",
    "- again, you can use `sklearn.model_selection.train_test_split`.\n",
    "- you might want to set the random seed to a fixed value for reproducibility.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d)\n",
    "\n",
    "Create three [DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) using the below provided `ReactionDataset` class:\n",
    "- `train_loader` for the training set, \n",
    "- `val_loader` for the validation set, and\n",
    "- `test_loader` for the test set.\n",
    "\n",
    "\n",
    "In `lab5-intro_pytorch.ipynb`, we directly iterated over the data without using a dataloader when training the toy model. \n",
    "But using a dataloader is more convenient and flexible, giving you more control over the data loading process.\n",
    "For a brief tutorial on using dataloader, see https://pytorch.org/tutorials/beginner/basics/data_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ReactionDataset(Dataset):\n",
    "    \"\"\"Reaction dataset.\n",
    "\n",
    "    Args:\n",
    "        X: The features.\n",
    "        y: The labels.\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y):\n",
    "\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e)\n",
    "\n",
    "Build an MLP model `MyMLP` with **two hidden layers** for the classification task. Hint: \n",
    "- refer to `lab5-intro_pytorch.ipynb` for a simple MLP model; note there is only one hidden layer in that example.\n",
    "\n",
    "The number of input nodes and output nodes of your MLP model should be fixed. What are they, and why? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (f)\n",
    "\n",
    "1. In part (e), what is the activation function you used for the hidden layers? Why you chose it over other options?\n",
    "\n",
    "2. Do you add an activation function to the output layer? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (g)\n",
    "\n",
    "\n",
    "Train your MLP model using the training set. Feel free to use the below provided `train_multi_epoch` function.\n",
    "\n",
    "- For classification, we use the cross entropy as the loss. In pytorch there are two cross entropy class: [BCELossWithLogits](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html) and [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html). Which one should we use for this problem? Why?\n",
    "- Use the `Adam` optimizer.\n",
    "- You can use `torch.manual_seed()` function to set the random seed for reproducibility (before initializing your model). Sometimes, the model performance can be sensitive to the random seed. If you find your model performance is not stable, you can try different random seeds to see if it helps.\n",
    "- Use the below hyperparameters:\n",
    "  - for both hidden layers, using 10 nodes\n",
    "  - learning rate: 0.001 (in optimizer)\n",
    "  - batch size: 32 (in dataloader)\n",
    "  - number of epochs (1 epoch means iterating over all data points in the training set once): 200 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_step(model, optimizer, loss_fn, X, y):\n",
    "    \"\"\"Train for a single step.\n",
    "\n",
    "    Args:\n",
    "        model: the model to train\n",
    "        optimizer: the optimizer to use\n",
    "        loss_fn: the loss function to use\n",
    "        X: the input data\n",
    "        y: the target data\n",
    "    \"\"\"\n",
    "    y_pred = model(X)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "def train_multiple_epochs(model,  optimizer, loss_fn, train_loader, X_train, y_train, X_val, y_val, num_epochs):\n",
    "    \"\"\"\n",
    "    Train the model for multiple epochs.\n",
    "\n",
    "    Args:\n",
    "        model: the model to train\n",
    "        optimizer: the optimizer to use\n",
    "        loss_fn: the loss function to use\n",
    "        train_loader: the training data loader\n",
    "        X_train: the entire training data\n",
    "        y_train: the entire training labels\n",
    "        X_val: the entire validation data\n",
    "        y_val: the entire validation labels\n",
    "        num_epochs: the total number of epochs to train for\n",
    "\n",
    "    Returns:\n",
    "        train_losses: the training losses for each epoch\n",
    "        val_losses: the validation losses for each epoch\n",
    "    \"\"\"\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for X_i, y_i in train_loader:\n",
    "            train_one_step(model, optimizer, loss_fn, X_i, y_i)\n",
    "\n",
    "        train_loss = loss_fn(model(X_train), y_train).item()\n",
    "        val_loss = loss_fn(model(X_val), y_val).item()\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (h)\n",
    "\n",
    "Plot the training loss and validation loss as a function of the epoch number. \n",
    "\n",
    "Based on the plot, what is the best epoch number to stop the training? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (i)\n",
    "\n",
    "Above in part (g), we used a set of hyperparameters to train the model. Now, we will try to find a better set of hyperparameters.\n",
    "\n",
    "- Tune the hyperparameters mentioned in part (g) to improve the model performance. You can try different values for each hyperparameter. Recall the best hyperparameters you found would be the ones that give the best performance on the validation set.  \n",
    "\n",
    "Can you find a set of hyperparameters that give a better performance than the one in part (g)? If yes, what are they? If not, why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (j)\n",
    "\n",
    "For the best model you found in part (i), evaluate its performance on the test set. If you cannot find a better model in part (i), use the model in part (g)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
